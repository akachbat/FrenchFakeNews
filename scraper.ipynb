{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (4.4.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (6.2.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (3.4.5)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from newspaper3k) (2.22.0)\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (41.4.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.9.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.24.2)\n",
      "Building wheels for collected packages: tinysegmenter, feedparser, feedfinder2, jieba3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13542 sha256=b3fb49439bfff8cbacf520a54768ef832b6bd2b6026da5f30e42f7e3a5379d14\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\81\\2b\\43\\a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for feedparser (setup.py): started\n",
      "  Building wheel for feedparser (setup.py): finished with status 'done'\n",
      "  Created wheel for feedparser: filename=feedparser-5.2.1-cp37-none-any.whl size=44944 sha256=d751b5ffc4ee854a9ec4e3c0fe88fe3146c4b97e31b7718531f84e54ed8d2cda\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\8c\\69\\b7\\f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3362 sha256=fb6374b2236c5bed842a7464576d506888f02133e06835f9713006c29f9fd2da\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\de\\03\\ca\\778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398414 sha256=41f1aaf583a622455cb347c204689bb4d9d576040caa7ad3a9246ac54892be08\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\83\\15\\9c\\a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "Successfully built tinysegmenter feedparser feedfinder2 jieba3k\n",
      "Installing collected packages: requests-file, tldextract, cssselect, tinysegmenter, feedparser, feedfinder2, jieba3k, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2570"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read links data\n",
    "\n",
    "df = pd.read_csv(\"links.csv\", delimiter='|', encoding=\"utf-8\", dtype=str)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3964.24 seconds'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        a = Article(row.URL, language='fr') \n",
    "        a.download()\n",
    "        a.parse()\n",
    "        a.nlp()\n",
    "    except:\n",
    "        print('Error scraping')\n",
    "        pass\n",
    "    df.at[index, 'domain'] = a.source_url\n",
    "    df.at[index, 'summary'] = a.summary\n",
    "    #.replace(\"\\n\", \" \")\n",
    "    df.at[index, 'content'] = a.text\n",
    "    df.at[index, 'title'] = a.title\n",
    "    df.at[index, 'top_image'] = a.top_image\n",
    "    df.at[index, 'authors'] = ', '.join([t for t in a.authors])\n",
    "    df.at[index, 'keywords'] = ', '.join([t for t in a.keywords])\n",
    "    df.at[index, 'meta_keywords'] = a.meta_keywords\n",
    "    df.at[index, 'meta_description'] = a.meta_description\n",
    "    df.at[index, 'tags'] = ', '.join([t for t in a.tags])\n",
    "    df.at[index, 'published_at'] = a.publish_date.strftime(\"%d-%m-%Y\") if a.publish_date else ''\n",
    "    df.at[index, 'scraped_at'] = datetime.datetime.utcnow().strftime(\"%d-%m-%Y\")\n",
    "    #wait 2 seconds\n",
    "    #time.sleep(1)    \n",
    "    \n",
    "#save data to file\n",
    "#df_test.to_csv('news_data.csv', sep=',', encoding=\"utf-8\")\n",
    "\n",
    "#show time spent for processing\n",
    "\"{0} seconds\".format(round(time.time() - startTime,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL                 0\n",
       "id                  0\n",
       "domain              0\n",
       "type                0\n",
       "summary             0\n",
       "content             0\n",
       "title               0\n",
       "top_image           0\n",
       "authors             0\n",
       "keywords            0\n",
       "meta_keywords       0\n",
       "meta_description    0\n",
       "tags                0\n",
       "published_at        0\n",
       "scraped_at          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Article(\"https://secretnews.fr/camping-4/\", language='fr') \n",
    "a.download()\n",
    "a.parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helo world...'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"helo world\"[0:255] + '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get content for https://secretnews.fr (not works with newscraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping\n",
      "Error scraping\n",
      "Error scraping\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.loc[(df['domain'] == 'https://secretnews.fr') & (df['content'] == \"\")].iterrows():\n",
    "    try:\n",
    "        html_file = requests.get(df.at[index, 'URL'])\n",
    "        soup = BeautifulSoup(html_file.text)\n",
    "        title = soup.select(\".entry-title\")[0].get_text(separator=\" \", strip=True)\n",
    "        post = soup.select(\".entry-content\")[0].get_text(separator=\" \", strip=True)\n",
    "        df.at[index, 'title'] = title\n",
    "        df.at[index, 'content'] = post\n",
    "        df.at[index, 'summary'] = post[0:255] + '...'\n",
    "    except:\n",
    "        print('Error scraping')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-64a5bb47cdaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'https://secretnews.fr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'summary'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for index, row in df.loc[df['domain'] == 'https://secretnews.fr']:            \n",
    "    df.at[index, 'content'] = df.at[index, 'content'][50:]\n",
    "    df.at[index, 'summary'] = df.at[index, 'content'][0:255] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('news_data_part_2.csv', sep=',', encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
